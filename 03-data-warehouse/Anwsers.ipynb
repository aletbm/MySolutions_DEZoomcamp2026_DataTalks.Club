{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 3 Homework: Data Warehousing & BigQuery\n",
        "\n",
        "## Loading the data\n",
        "\n",
        "You can use the following scripts to load the data into your GCS bucket:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UsUZobVduL7l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import tzdata, os\n",
        "\n",
        "os.environ[\"BUCKET_URL\"] = \"gs://ny_taxi_aletbm\"\n",
        "\n",
        "with open(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]) as f:\n",
        "    os.environ[\"DESTINATION__CREDENTIALS\"] = json.dumps(json.load(f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Alexander\\AppData\\Local\\Temp\\ipykernel_11292\\179648397.py:22: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tarfile.open(tz_path).extractall(folder)\n"
          ]
        }
      ],
      "source": [
        "def download_tzdata_windows(\n",
        "    base_dir=None,\n",
        "    year=2022,\n",
        "    name=\"tzdata\"\n",
        "):\n",
        "    import os\n",
        "    import tarfile\n",
        "    import urllib3\n",
        "\n",
        "    http = urllib3.PoolManager()\n",
        "    folder = base_dir if base_dir else os.path.join(os.path.expanduser('~'), \"Downloads\")\n",
        "    tz_path = os.path.join(folder, \"tzdata.tar.gz\")\n",
        "    \n",
        "    with open(tz_path, \"wb\") as f:\n",
        "        f.write(http.request('GET', f'https://data.iana.org/time-zones/releases/tzdata{year}f.tar.gz').data)\n",
        "    \n",
        "    folder = os.path.join(folder, name)\n",
        "    \n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "    \n",
        "    tarfile.open(tz_path).extractall(folder)\n",
        "    \n",
        "    with open(os.path.join(folder, \"windowsZones.xml\"), \"wb\") as f:\n",
        "        f.write(http.request('GET', f'https://raw.githubusercontent.com/unicode-org/cldr/master/common/supplemental/windowsZones.xml').data)\n",
        "download_tzdata_windows(year=2022)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lYh7r1mTf4uo"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "import requests\n",
        "import pandas as pd\n",
        "from dlt.destinations import filesystem\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76zT1PzAgs7A"
      },
      "source": [
        "Ingesting parquet files to GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xya0215jsnsb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline rides_pipeline load step completed in 1 minute and 3.57 seconds\n",
            "1 load package(s) were loaded to destination filesystem and into dataset rides_dataset\n",
            "The filesystem destination used gs://ny_taxi_aletbm location to store data\n",
            "Load package 1770651736.0392327 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "# Define a dlt source to download and process Parquet files as resources\n",
        "@dlt.source(name=\"rides\")\n",
        "def download_parquet():\n",
        "    prefix = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata\"\n",
        "    for month in range(1, 7):\n",
        "        file_name = f\"yellow_tripdata_2024-0{month}.parquet\"\n",
        "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        df = pd.read_parquet(BytesIO(response.content))\n",
        "\n",
        "        # Return the dataframe as a dlt resource for ingestion\n",
        "        yield dlt.resource(df, name=file_name)\n",
        "\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"rides_pipeline\",\n",
        "    destination=filesystem(layout=\"{schema_name}/{table_name}.{ext}\"),\n",
        "    dataset_name=\"rides_dataset\",\n",
        ")\n",
        "\n",
        "# Run the pipeline to load Parquet data into DuckDB\n",
        "load_info = pipeline.run(download_parquet(), loader_file_format=\"parquet\")\n",
        "\n",
        "# Print the results\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1. Counting records\n",
        "\n",
        "What is count of records for the 2024 Yellow Taxi Data?\n",
        "- 65,623\n",
        "- 840,402\n",
        "- 20,332,093\n",
        "- 85,431,289\n",
        "\n",
        "### Querys\n",
        "\n",
        "Create an schema:\n",
        "```sql\n",
        "CREATE SCHEMA `[PROJECT_ID].[DATABASE]`\n",
        "OPTIONS (\n",
        "  location = \"US\"\n",
        ");\n",
        "```\n",
        "\n",
        "Create an external table using the Yellow Taxi Trip Records:\n",
        "```sql\n",
        "CREATE OR REPLACE EXTERNAL TABLE `[PROJECT_ID].[DATABASE].yellow_taxi_external`\n",
        "OPTIONS (\n",
        "  format = 'PARQUET',\n",
        "  uris = [\n",
        "    'gs://[BUCKET_NAME]/rides_dataset/rides/yellow_tripdata_2024_*.parquet'\n",
        "  ]\n",
        ");\n",
        "```\n",
        "\n",
        "Create a (regular/materialized) table in BQ using the Yellow Taxi Trip Records\n",
        "```sql\n",
        "CREATE OR REPLACE TABLE `[PROJECT_ID].[DATABASE].yellow_taxi_2024`\n",
        "AS \n",
        "SELECT * FROM `[PROJECT_ID].[DATABASE].yellow_taxi_external`;\n",
        "```\n",
        "\n",
        "Count of records:\n",
        "```sql\n",
        "SELECT COUNT(*) FROM `[PROJECT_ID].[DATABASE].yellow_taxi_external`;\n",
        "```\n",
        "\n",
        "Anwser: 20,332,093"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2. Data read estimation\n",
        "\n",
        "Write a query to count the distinct number of PULocationIDs for the entire dataset on both the tables.\n",
        " \n",
        "What is the **estimated amount** of data that will be read when this query is executed on the External Table and the Table?\n",
        "\n",
        "- 18.82 MB for the External Table and 47.60 MB for the Materialized Table\n",
        "- 0 MB for the External Table and 155.12 MB for the Materialized Table\n",
        "- 2.14 GB for the External Table and 0MB for the Materialized Table\n",
        "- 0 MB for the External Table and 0MB for the Materialized Table\n",
        "\n",
        "### Querys\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "  COUNT(DISTINCT pu_location_id) AS distinct_pu_locations\n",
        "FROM `[PROJECT_ID].[DATABASE].yellow_taxi_external`;\n",
        "```\n",
        "\n",
        "```sql\n",
        "SELECT\n",
        "  COUNT(DISTINCT pu_location_id) AS distinct_pu_locations\n",
        "FROM `[PROJECT_ID].[DATABASE].yellow_taxi_2024`;\n",
        "```\n",
        "\n",
        "Anwser: 0 MB for the External Table and 155.12 MB for the Materialized Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3. Understanding columnar storage\n",
        "\n",
        "Write a query to retrieve the PULocationID from the table (not the external table) in BigQuery. Now write a query to retrieve the PULocationID and DOLocationID on the same table.\n",
        "\n",
        "Why are the estimated number of Bytes different?\n",
        "- BigQuery is a columnar database, and it only scans the specific columns requested in the query. Querying two columns (PULocationID, DOLocationID) requires \n",
        "reading more data than querying one column (PULocationID), leading to a higher estimated number of bytes processed.\n",
        "- BigQuery duplicates data across multiple storage partitions, so selecting two columns instead of one requires scanning the table twice, \n",
        "doubling the estimated bytes processed.\n",
        "- BigQuery automatically caches the first queried column, so adding a second column increases processing time but does not affect the estimated bytes scanned.\n",
        "- When selecting multiple columns, BigQuery performs an implicit join operation between them, increasing the estimated bytes processed\n",
        "\n",
        "### Querys\n",
        "\n",
        "Query to retrieve the PULocationID from the table:\n",
        "```sql\n",
        "SELECT pu_location_id\n",
        "FROM `[PROJECT_ID].[DATABASE].yellow_taxi_2024`\n",
        "LIMIT 10;\n",
        "```\n",
        "\n",
        "Query to retrieve the PULocationID and DOLocationID on the same table\n",
        "```sql\n",
        "SELECT pu_location_id, do_location_id\n",
        "FROM `[PROJECT_ID].[DATABASE].yellow_taxi_2024`\n",
        "LIMIT 10;\n",
        "```\n",
        "Anwser: BigQuery is a columnar database, and it only scans the specific columns requested in the query. Querying two columns (PULocationID, DOLocationID) requires \n",
        "reading more data than querying one column (PULocationID), leading to a higher estimated number of bytes processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4. Counting zero fare trips\n",
        "\n",
        "How many records have a fare_amount of 0?\n",
        "- 128,210\n",
        "- 546,578\n",
        "- 20,188,016\n",
        "- 8,333\n",
        "\n",
        "### Query\n",
        "\n",
        "```sql\n",
        "SELECT COUNT(*) AS zero_fare_count\n",
        "FROM `[PROJECT_ID].[DATABASE].yellow_taxi_2024`\n",
        "WHERE fare_amount = 0;\n",
        "```\n",
        "\n",
        "Anwser: 8,333"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 5. Partitioning and clustering\n",
        "\n",
        "What is the best strategy to make an optimized table in Big Query if your query will always filter based on tpep_dropoff_datetime and order the results by VendorID (Create a new table with this strategy)\n",
        "\n",
        "- Partition by tpep_dropoff_datetime and Cluster on VendorID\n",
        "- Cluster on by tpep_dropoff_datetime and Cluster on VendorID\n",
        "- Cluster on tpep_dropoff_datetime Partition by VendorID\n",
        "- Partition by tpep_dropoff_datetime and Partition by VendorID\n",
        "\n",
        "Anwser: Partition by tpep_dropoff_datetime and Cluster on VendorID\n",
        "\n",
        "## Explanation\n",
        "+ Partitioning:\n",
        "    + BigQuery partitions are used to filter large datasets efficiently.\n",
        "    + Since your queries always filter on tpep_dropoff_datetime, partitioning by this column reduces the number of bytes scanned, making queries faster and cheaper.\n",
        "\n",
        "+ Clustering:\n",
        "    + Clustering organizes the data within each partition based on the values of one or more columns.\n",
        "    + Because your queries order results by VendorID, clustering on VendorID improves sorting and aggregation performance.\n",
        "\n",
        "```sql\n",
        "CREATE OR REPLACE TABLE `[PROJECT_ID].[DATABASE].yellow_taxi_optimized`\n",
        "PARTITION BY DATE(tpep_dropoff_datetime)\n",
        "CLUSTER BY vendor_id AS\n",
        "SELECT *\n",
        "FROM `[PROJECT_ID].[DATABASE].yellow_taxi_2024`;\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 6. Partition benefits\n",
        "\n",
        "Write a query to retrieve the distinct VendorIDs between tpep_dropoff_datetime\n",
        "2024-03-01 and 2024-03-15 (inclusive)\n",
        "\n",
        "Use the materialized table you created earlier in your from clause and note the estimated bytes. Now change the table in the from clause to the partitioned table you created for question 5 and note the estimated bytes processed. What are these values? \n",
        "\n",
        "Choose the answer which most closely matches.\n",
        " \n",
        "- 12.47 MB for non-partitioned table and 326.42 MB for the partitioned table\n",
        "- 310.24 MB for non-partitioned table and 26.84 MB for the partitioned table\n",
        "- 5.87 MB for non-partitioned table and 0 MB for the partitioned table\n",
        "- 310.31 MB for non-partitioned table and 285.64 MB for the partitioned table\n",
        "\n",
        "### Querys:\n",
        "\n",
        "Query to retrieve the distinct VendorIDs between tpep_dropoff_datetime\n",
        "2024-03-01 and 2024-03-15 on materialized table:\n",
        "```sql\n",
        "SELECT DISTINCT vendor_id\n",
        "FROM `[PROJECT_ID].[DATABASE].ny_taxi.yellow_taxi_2024`\n",
        "WHERE DATE(tpep_dropoff_datetime) BETWEEN '2024-03-01' AND '2024-03-15';\n",
        "```\n",
        "\n",
        "Query to retrieve the distinct VendorIDs between tpep_dropoff_datetime\n",
        "2024-03-01 and 2024-03-15 on partitioned table:\n",
        "```sql\n",
        "SELECT DISTINCT vendor_id\n",
        "FROM `[PROJECT_ID].[DATABASE].ny_taxi.yellow_taxi_optimized`\n",
        "WHERE DATE(tpep_dropoff_datetime) BETWEEN '2024-03-01' AND '2024-03-15';\n",
        "```\n",
        "\n",
        "Anwser: 310.24 MB for non-partitioned table and 26.84 MB for the partitioned table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 7. External table storage\n",
        "\n",
        "Where is the data stored in the External Table you created?\n",
        "\n",
        "- Big Query\n",
        "- Container Registry\n",
        "- GCP Bucket\n",
        "- Big Table\n",
        "\n",
        "Answer: GCP Bucket\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "+ When you create an External Table in BigQuery, BigQuery does not copy the data into its own storage.\n",
        "+ Instead, it references the data where it lives, in your case in Parquet files on GCS.\n",
        "+ Queries on the External Table read the data directly from GCS each time they run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 8. Clustering best practices\n",
        "\n",
        "It is best practice in Big Query to always cluster your data:\n",
        "- True\n",
        "- False\n",
        "\n",
        "Clustering is best practice only when your queries benefit from it, not always.\n",
        "Answer: False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Question 9. Understanding table scans\n",
        "\n",
        "No Points: Write a `SELECT count(*)` query FROM the materialized table you created. How many bytes does it estimate will be read? Why?\n",
        "\n",
        "### Query\n",
        "\n",
        "```sql\n",
        "SELECT COUNT(*) FROM `poetic-genius-486912-k5.ny_taxi.yellow_taxi_2024`;\n",
        "```\n",
        "\n",
        "It shows 0 B because BigQuery returned the result from cached query results. To see the actual bytes processed, cached results must be disabled.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "03-data-warehouse",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
